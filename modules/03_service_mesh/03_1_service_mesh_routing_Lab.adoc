:noaudio:
:scrollbar:
:data-uri:
:toc2:
:linkattrs:
:transaction_costs: link:https://en.wikipedia.org/wiki/Transaction_cost[transaction costs]

= Service Mesh Routing Lab

.Goals
* Inject Red Hat Service Mesh Envoy proxy configurations into an API gateway
* Configure an Red Hat Service Mesh Egress Route for an API gateway
* Invocation of your backend service via the Red Hat Service Mesh Ingress gateway and Red Hat 3scale API Management gateway

:numbered:

== Overview

Traffic routing can occur in several locations of the software stack:

. *OpenShift Container Platform*:
+
Provides routing functionality via either the HA-Proxy router and the traditional Kubernetes _Ingress_ controller.
Both of these components provide the ability to handle the traffic that enters the cluster from the outside.
+
OCP also provides the ability to completely by-pass these controllers via the use of a _NodePort_. 
+
Once traffic is in the OCP cluster, OCP also provides traffic routing features at the _service_ layer.

. *Red Hat Service Mesh*:
+
Red Hat Service Mesh replaces the Kubernetes Ingress controller with a new _Gateway_ and _Virtual Service_ that work in tandem to route the traffic into the mesh.

. *3scale API Management*:
+
The API Gateway is configured with policies that (among other capabilities) will route traffic to backend business services given the _Host_ header of the request.

In this lab, you work through various scenarios with the goal of gaining appreciation for the traffic routing features of each of these mentioned tools and how they complement each other.

=== Reference

. link:https://blog.jayway.com/2018/10/22/understanding-istio-ingress-gateway-in-kubernetes/[Understanding Red Hat Service Mesh Ingress Gateway]
. link:https://itnext.io/how-to-access-your-app-behind-an-openshift-router-87cbae3e7185[Accessing apps behind OCP Router]

== Lock down Red Hat Service Mesh
This first section of the lab involves you impersonating the role of a cluster admin to lock down the pre-installed Red Hat Service Mesh.

. Service Mesh has an installation option that configures the Red Hat Service Mesh proxy to pass through, instead of block, calls to external services on any ports. 
This option is enabled by default so that one can start evaluating Red Hat Service Mesh quickly, without controlling access to external services.
For the purpose of this lab, modify this default behavior such that apps can not make requests to external services by default
+
Do so by executing one of the following:
+
-----
$ oc get configmap istio -n istio-system --export -o yaml \
     | sed 's/mode: ALLOW_ANY/mode: REGISTRY_ONLY/g' \
     | oc replace -n istio-system --as=system:admin -f - 
-----


== API Gateway: Red Hat Service Mesh enabled

=== Overview

As the next step in the journey toward API management and service mesh, in this section of the lab you will inject your API gateway with the _Envoy_ sidecar proxy from Red Hat Service Mesh.
A service mesh enabled API Gateway is of limited value on its own, but you will continue to build upon the foundations set here in later sections of this lab.

image::images/deployment_apicast-istio.png[]

In the above diagram, notice the introduction of a new pod: _prod-apicast-istio_.
Inbound requests through the API gateway production route are now directed to this new API gateway pod injected with the  _envoy_ sidecar.

The _Envoy_ sidecar in your _prod-apicast-istio_ pod will interoperate with _service mesh control plane_ functionality found in the _istio-system_ namespace.

Your API gateway will continue to pull _service proxy_ configurations from the pre-provisioned multi-tenant API Manager using the value of their  _$THREESCALE_PORTAL_ENDPOINT_ environment variable.

=== Inject Red Hat Service Mesh proxy side-car

. Install community _istioctl_ utility
+
-----
$ mkdir -p $HOME/bin/ && \
    wget https://github.com/istio/istio/releases/download/1.1.0-rc.2/istio-1.1.0-rc.2-linux.tar.gz -O /tmp/istioctl.tar.gz && \
    tar -zxvf /tmp/istioctl.tar.gz -C /tmp && \
    cp /tmp/istio-1.1.0-rc.2/bin/istioctl $HOME/bin

-----

. Retrieve the YAML representation of the current API gateway production deployment:
+
-----
$ oc get deploy prod-apicast -n $GW_PROJECT -o yaml > $HOME/lab/prod-apicast.yml
-----
+
You will use the definition of your existing API gateway as a template to create a new Red Hat Service Mesh enabled API gateway.

. Differentiate your Red Hat Service Mesh enabled API gateway from your existing API gateway:
+
-----
$ sed -i "s/prod-apicast/$OCP_USERNAME-prod-apicast-istio/" $HOME/lab/prod-apicast.yml
-----
.. The reason you have included $OCP_USERNAME in the name of your  service mesh enabled API gateway is because you will need to differentiate it from other service mesh enabled API gateways that may be managed in the same service mesh.
.. Also, the _observability_ user interfaces included in Red Hat Service Mesh such as Jaeger are not currently multi-tenant.
+
By providing a unique identifer as a prefix to your API gateway name, you will be more easily able to identify logs and traces amongst others on the system.

. Place the deployment in a paused state:
+
-----
$ sed -i "s/replicas:\ 1/replicas: 1\n  paused: true/" $HOME/lab/prod-apicast.yml
-----

. Specify to the gateway that it should pull API service configuration information from the API Manager upon first request:
+
-----
$ sed -i "s/boot/lazy/" $HOME/lab/prod-apicast.yml
-----

. View the configmap in the `istio-system` project:
+
-----
$ oc describe configmap istio -n istio-system | more
-----
+
Your OCP user has already been enabled with _view_ access to the _istio-system_ namespace.
This provides access to the _istio_ configuration map.
The _istio_ configmap was generated by a cluster administrator when the Red Hat Service Mesh control plane was installed on OCP.

. Inject Red Hat Service Mesh configs (from the _istio_ configmap) into a new API gateway deployment:
+
-----

$ istioctl kube-inject \
           -f $HOME/lab/prod-apicast.yml \
           > $HOME/lab/prod-apicast-istio.yml
-----
+
NOTE:  For the purpose of this lab, you have now manually injected Red Hat Service Mesh related configs into a _deployment_ definition.
An alternative approach which involves adding a special annotation to your deployment definition will be introduced later in this lab.

. View the Red Hat Service Mesh injected API gateway deployment descriptor:
+
-----
$ less $HOME/lab/prod-apicast-istio.yml | more
-----

. Deploy a new Red Hat Service Mesh enabled API gateway production gateway:
+
-----
$ oc create \
     -f $HOME/lab/prod-apicast-istio.yml \
     -n $GW_PROJECT
-----

/////
Appears that with Red Hat Service Mesh, these limits and requests on istio containers are included out of the box

. Inject required resource limits and requests into Red Hat Service Mesh related containers :
+
There is a cluster quota assigned to your OCP user.
This cluster quota requires that all containers, including the _istio-proxy_ and _istio-init_, specify _limits_ and _requests_.
+
-----
$ oc patch deploy/$OCP_USERNAME-prod-apicast-istio -n $GW_PROJECT\
   --patch '{"spec":{"template":{"spec":{"containers":[{"name":"istio-proxy", "resources": {   "limits":{"cpu": "500m","memory": "128Mi"},"requests":{"cpu":"50m","memory":"32Mi"}   }}]}}}}'

$ oc patch deploy/$OCP_USERNAME-prod-apicast-istio -n $GW_PROJECT \
   --patch '{"spec":{"template":{"spec":{"initContainers":[{"name":"istio-init", "resources": {   "limits":{"cpu": "500m","memory": "128Mi"},"requests":{"cpu":"50m","memory":"32Mi"}   }}]}}}}'
-----
/////

. Allow pods in the $GW_PROJECT to run using any user, including root:
+
-----
$ oc adm policy add-scc-to-user anyuid -z default -n $GW_PROJECT --as=system:admin
-----

.. For your new Red Hat Service Mesh enabled API gateway pod to start, it needs the _anyuid_ sidecar container (SCC).
+
The reason for this is that the _envoy_ sidecar containers from Red Hat Service Mesh currently run as a specific user ID.
Unlike most middleware containers that can run using any arbitrary user ID that is assigned to them at runtime by OCP, the _envoy_ sidecar containers would immediately fail upon startup without the _anyuid_ SCC.
If you attempted to do so, you would see an error similiar to the following:
+
-----
Error creating: pods "user50-prod-apicast-istio-6456c879c8" is forbidden: unable to validate against any security context constraint: [spec.initContainers[0].securityContext.privileged: Invalid value: true: Privileged containers are not allowed capabilities.add: Invalid value: "NET_ADMIN": capability may not be added spec.initContainers[0].securityContext.privileged: Invalid value: true: Privileged containers are not allowed capabilities.add: Invalid value: "NET_ADMIN":
-----

.. For the purpose of this lab, the cluster administrator of your OCP environment has provided you with the ability to _impersonate_ the cluster administrator.
Doing so provides you with the ability to apply the _anyuid_ SCC to your project.

. Allow pods in the $GW_PROJECT to run in a privledged manner:
+
-----
$ oc adm policy add-scc-to-user privileged -z default -n $GW_PROJECT --as=system:admin
-----
+
This SCC is required to ensure changes to the podâ€™s networking configuration is updated successfully with the istio-init initialization container.


. Resume the paused deployment:
+
-----
$ oc rollout resume deploy/$OCP_USERNAME-prod-apicast-istio -n $GW_PROJECT
-----

.. Notice the presence of an additional container in your new pod.  This additional container is the Red Hat Service Mesh proxy sidecar.
.. Both containers in the new pod should have started and the pod should be in a _Running_ state:
+
-----
$ oc get pods -n $GW_PROJECT


NAME                                         READY     STATUS    RESTARTS   AGE

....

user50-prod-apicast-istio-784dc96c75-gvh5f   2/2       Running   0          5m

-----
+
If either of the containers did not start up and the _READY_ column indicates anything other than _2/2_, this indicates a problem.
It's likely that _liveness_ and/or _readiness_ probes on the API gateway are failing.
It's possible that this is due to a misconfiguration of Red Hat Service Mesh.
As an initial troubleshooting step, remove the _liveness_ and _readiness_ probes defined in the deployment.
After doing so, do both containers start ?
If so, you'll need to troubleshoot the root cause of why either the _liveness_ probe, _readiness_ probe, or both probes are failing.

. Modify the _prod-apicast_ service to route inbound requests to the new Red Hat Service Mesh enabled _apicast_:
+
-----
$ oc patch service/prod-apicast -n $GW_PROJECT \
   --patch '{"spec":{"selector":{"app":"'$OCP_USERNAME'-prod-apicast-istio"}}}'
-----
+
Notice that the _service_ to your backend catalog application is being modified instead of the OpenShift _route_.
You have essentially conducted a simple _A / B Deployment_ at the _service_ layer where 100% of all traffic through the existing unmodified route will flow to this new Red Hat Service Mesh enabled pod.
As an alternative, _A / B Deployment_ could have occured at the _route_ layer.
This latter approach would have required you to create an additional _service_ associated with the new Red Hat Service Mesh enabled pod.


=== Test 
. Verify your `$CATALOG_USER_KEY` environment variable is set:
+
-----
$ echo $CATALOG_USER_KEY

d59904ad4515522ecccb8b81c761a283
-----

. From the terminal, execute the following:
+
-----
$ curl -v -k `echo "https://"$(oc get route/catalog-prod-apicast-$OCP_USERNAME -n $GW_PROJECT -o template --template {{.spec.host}})"/products?user_key=$CATALOG_USER_KEY"`
-----

.. [red]#The response should be an HTTP 404.#
.. Why would this be the case ?
... Inspect the API gateway log file for any clues.
... Is the request reaching your new Red Hat Service Mesh enabled API gateway?
... The root problem is that your service mesh enabled API gateway is unable to connect to the _system-provider_ endpoint exposed by the remote multi-tenant API Manager via the value of: $THREESCALE_PORTAL_ENDPOINT.
+
Your API gateway needs to do this to retrieve all the policy management configuration data from the API Manager.
The reason your API gateway can not make a connection to the API Manager is that $THREESCALE_PORTAL_ENDPOINT references an external internet URL.
By default, Red Hat Service Mesh blocks all outbound requests to the internet.
In the next section, you will define an _egress route_ to allow your API gateway to communicate with the API Manager.

.  Isolate the problem with your new  service mesh enabled API gateway by testing the call to the _system-provider_ of the API Manager, from within the API gateway.

.. Execute the following to test the call:
+
-----
$ oc rsh `oc get pod -n $GW_PROJECT | grep "apicast-istio" | awk '{print $1}'` \
          curl -v -k ${THREESCALE_PORTAL_ENDPOINT}/admin/api/services.json


...

Defaulting container name to user50-prod-apicast-istio.
Use 'oc describe pod/user50-prod-apicast-istio-784dc96c75-vxxz5 -n rhte-mw-api-mesh-50' to see all of the containers in this pod.
* About to connect() to user50-3scale-mt-admin.apps.8091.openshift.opentlc.com port 443 (#0)
*   Trying 52.7.161.237...
* Connected to user50-3scale-mt-admin.apps.8091.openshift.opentlc.com (52.7.161.237) port 443 (#0)
* Initializing NSS with certpath: sql:/etc/pki/nssdb
* NSS error -5938 (PR_END_OF_FILE_ERROR)
* Encountered end of file
* Closing connection 0
curl: (35) Encountered end of file
command terminated with exit code 35
-----

.. If you received an error response similar to above, you have isolated the problem to an inability for your API gateway pod to make an external call out of the service mesh.


=== Apply Custom _Egress Route_

In this section, you create a custom Red Hat Service Mesh _ServiceEntry_ that allows your API gateway to connect to the _system-provider_ of the multi-tenant API Manager.

. Create a custom Red Hat Service Mesh _Egress Route_ for API gateway configuration file:
+
-----
$ echo \
    "apiVersion: networking.istio.io/v1alpha3
kind: ServiceEntry
metadata:
  name: $OCP_USERNAME-catalog-apicast-egress-rule
spec:
  hosts:
  - $TENANT_NAME-admin.$API_WILDCARD_DOMAIN
  location: MESH_EXTERNAL
  ports:
  - name: https-443
    number: 443
    protocol: HTTPS
  resolution: DNS" \
 > $HOME/lab/catalog-apicast-egressrule.yml
-----

.. Note the value of `spec -> hosts` is set to the same value of the $THREESCALE_PORTAL_ENDPOINT specified in your Red Hat 3scale API Management gateway.
.. This should allow your API gateway to connect to the route that exposes the _system-provider_ service of the multi-tenant API Manager.


. Load the new egress rule:
+
-----
$ oc create -f $HOME/lab/catalog-apicast-egressrule.yml -n $GW_PROJECT 
-----

. View new ServiceEntry:
+
-----
$ oc describe serviceentry $OCP_USERNAME-catalog-apicast-egress-rule -n $GW_PROJECT
-----

. Now that a custom _egress route_ has been added, your API gateway should be able to pull configuration data from the API Manager.
+
Use a command like the following to verify that your service mesh enabled API gateway can now poll the API Manager for proxy service configuration information:
+
-----
$ oc rsh `oc get pod -n $GW_PROJECT | grep "apicast-istio" | awk '{print $1}'` \
     curl -k ${THREESCALE_PORTAL_ENDPOINT}/admin/api/services.json \
     | python -m json.tool | more

...

{
    "services": [
        {
            "service": {
                "backend_version": "1",
                "created_at": "2018-08-07T11:13:03Z",
                "end_user_registration_required": true,
                "id": 3,
                "links": [
                    {
                        "href": "https://user1-3scale-admin.apps.7777.thinkpadratwater.com/admin/api/services/3/metrics",
                        "rel": "metrics"
                    },


....
-----

. Either wait up to 5 minutes for your service mesh enabled API gateway to refresh its proxy configuration (because pulling this configuration data previously failed) or restart the pod.
. Using the curl utility, attempt again to retrieve catalog data via your service mesh enabled API gateway:
+
-----
$ curl -v -k `echo "https://"$(oc get route/catalog-prod-apicast-$OCP_USERNAME -n $GW_PROJECT -o template --template {{.spec.host}})"/products?user_key=$CATALOG_USER_KEY"`
-----
+
This time, you should see the catalog data in the response.
This request now flows through your service mesh enabled API gateway.

[blue]#As mentioned previously, what you have accomplished so far is of limited value on its own. 
However, this is a step to full utilization of API management and service mesh of your container native application.
In the next sections of this lab you will continue to build upon this foundation.#


== Red Hat Service Mesh Ingress Gateway

Until now, traffic into the production API gateway has been directly via the standard _HAProxy_-based OCP _router_.

This has worked, however what is missing is the ability to apply more sophisticated route rules and capture improved _observability_ as soon as the request enters the OCP cluster.
Red Hat Service Mesh includes an _Ingress_ gateway that implements these additional features.

In this section of the lab, you will modify the flow of inbound traffic so that it flows through the _Ingress_ gateway of Red Hat Service Mesh.
Among other benefits, you will be able to trace inbound traffic as it initially enters your solution.

You will do so using the following two approaches:

. Route inbound request directly to the service mesh Ingress Gateway via an OCP _NodePort_.
. Route inbound request to the service mesh Ingress Gateway via its _route_ configuration in OCP's HA-Proxy router.


=== Define Gateway and VirtualService

. Create an environment variable that reflects the _Production Public Base URL_ of the _catalog-service_ configured in the API Manager:
+
-----
$ echo "export CATALOG_API_GW_HOST=`oc get route/catalog-prod-apicast-$OCP_USERNAME -n $GW_PROJECT -o template --template {{.spec.host}}`" >> ~/.bashrc

$ source ~/.bashrc
-----
+
The value of $CATALOG_API_GW_HOST will be specified in the request to the Red Hat Service Mesh ingress gateway as an HTTP _HOST_ header.
It should match what has been configured in the _catalog-service_ in your API Manager.
+
image::images/recall_prod_base.png[]


. Create a file that defines an _Ingress_ gateway:
+
-----
$ echo \
    "apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: catalog-istio-gateway
spec:
  selector:
    istio: ingressgateway # use istio default controller
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "$CATALOG_API_GW_HOST"" \
 > $HOME/lab/catalog-istio-gateway.yml
-----

. Create the gateway in OCP:
+
-----
$ oc create -f $HOME/lab/catalog-istio-gateway.yml -n $GW_PROJECT
-----

. Create a file that defines a virtual service:
+
-----
$ echo \
    "apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: catalog-istio-gateway-vs
spec:
  hosts:
  - "$CATALOG_API_GW_HOST"
  gateways:
  - catalog-istio-gateway
  http:
  - match:
    - uri:
        prefix: /products
    route:
    - destination:
        port:
          number: 8080
        host: prod-apicast" \
> $HOME/lab/catalog-istio-gateway-vs.yml
-----

. Create the virtual service in OCP:
+
-----
$ oc create -f $HOME/lab/catalog-istio-gateway-vs.yml -n $GW_PROJECT 
-----

-----
# sudo -i


# oc project istio-system && \
         oc rsh `oc get pod | grep "istio-ingressgateway" | awk '{print $1}'` \
         curl http://localhost:15000/config_dump \
         > /tmp/config_dump \
         && less /tmp/config_dump \
         | jq ".configs | last | .dynamic_route_configs"



# exit
-----

=== Direct inbound traffic via OCP Node Port

image::images/ingress_gateway_via_node_port.png[]

. Set environment variables that reflect the host and port of the Red Hat Service Mesh ingress gateway service:
+
-----
$ echo "export INGRESS_HOST=$(oc -n istio-system get service istio-ingressgateway -o jsonpath='{.status.loadBalancer.ingress[0].ip}')" >> ~/.bashrc

$ echo "export INGRESS_PORT=$(oc -n istio-system get service istio-ingressgateway -o jsonpath='{.spec.ports[?(@.name=="http2")].port}')" >> ~/.bashrc

source ~/.bashrc
-----
+
NOTE:  This $INGRESS_HOST:$INGRESS_PORT socket is local to the Red Hat OpenShift Container Platform subnet.
It only becomes available when there is an Red Hat Service Mesh gateway and virtual service present and even then it might take up to 5 minutes to become available.

. Smoke test a request for catalog data via the newly configured Red Hat Service Mesh ingress gateway:
+
-----
$ curl -v \
       http://$INGRESS_HOST:$INGRESS_PORT/products?user_key=$CATALOG_USER_KEY
-----

.. The response back should be a 404.  Why ?
.. Check the log file of your istio enabled production API gateway.  There should be a recent statement similar to the following
+
-----
host_based_finder.lua:20: find_service(): service not found for host istio-ingressgateway-istio-system.apps-326b.generic.opentlc.com, client: 127.0.0.1, server: _, request: "GET /products?user_key=95706a476fcf08a95322084a8f5632b4 HTTP/1.1", host: "istio-ingressgateway-istio-system.apps-326b.generic.opentlc.com"
-----
.. The HTTP request was routed from the istio ingressgateway to the istio enabled API gateway.
The API gateway is not able to deduce from the request the _Production Base URL_ that corresponds to a API Managed service (such as your catalog service).

. Smoke test a request for catalog data via the newly configured Red Hat Service Mesh ingress gateway:
+
-----
$ curl -v \
       -HHost:$CATALOG_API_GW_HOST \
       http://$INGRESS_HOST:$INGRESS_PORT/products?user_key=$CATALOG_USER_KEY
-----
+
Notice the use of the HTTP _Host_ header.

.. Why is it needed ?
.. What happens if this _HOST_ header is not included in the request ?
.. What happens if this _HOST_ header is included in the request but its value (along with the _host_ attributes in the gateway and virtual service) do not match the _Production Public Base Url_ of the _catalog-service_ in the API Manager ?


=== Direct inbound traffic via OCP Route

image::images/ingress_gateway_via_node_port.png[]

. The previous smoke test used an $INGRESS_HOST and $INGRESS_PORT that are made available on the local Red Hat OpenShift Container Platform network subnet.
Usage of this local $INGRESS_HOST only worked because your curl client is on the same host as Red Hat OpenShift Container Platform.
+
The `istio-system` namespace provides a route called: `istio-ingressgateway`.
Like all Red Hat OpenShift Container Platform routes, it is exposed to external clients.
+
Try invoking the API gateway via this `istio-ingressgateway` route instead of using the internal _service-mesh_ related $INGRESS_HOST and $INGRESS_PORT.
The results should be the same.  The command to use is as follows:
+
-----
$  curl -v \
       -HHost:$CATALOG_API_GW_HOST \
      `echo "http://"$(oc get route istio-ingressgateway -n istio-system -o template --template {{.spec.host}})"/products?user_key=$CATALOG_USER_KEY"`
-----
+
NOTE: [red]#There is currently a bug in your student lab environment that is causing this step to fail.  
Specifically, the _istio-ingressgateway_ service of the _istio-system_ namespace makes use of OCP _nodeport_ features.
The _nodeports_ defined in the _istio-ingressgateway_ service are currently blocked by the cloud provider hosting your VM#

ifdef::showscript[]

Deletion of the route causes the curl command (using the HOST) to no longer work.  not sure why.

. The original route to the product API Gateway specific to the _catalog_ service can optionally be deleted.
Going forward all incoming traffic will flow through the Red Hat Service Mesh Ingress Gateway.
+
-----
$ oc delete route/catalog-prod-apicast-$OCP_USERNAME -n $GW_PROJECT
-----

endif::showscript[]



ifdef::showscript[]
endif::showscript[]
